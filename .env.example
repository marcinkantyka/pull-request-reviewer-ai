# Local PR Reviewer Configuration
# Copy this file to .env and customize as needed

# ============================================================================
# MODEL CONFIGURATION (M3 16GB Optimized)
# ============================================================================

# AI Model to use for code review
# For M3 16GB: Use 7B model (4-5GB RAM)
# Options:
#   - qwen2.5-coder:7b (recommended for 16GB RAM)
#   - qwen2.5-coder:14b (requires 32GB RAM)
#   - qwen2.5-coder:32b (requires 64GB RAM)
MODEL_NAME=qwen2.5-coder:7b

# ============================================================================
# REPOSITORY CONFIGURATION
# ============================================================================

# Path to your git repository
# Use . for current directory or absolute path
REPO_PATH=.

# Base branch for PR diffs
# Usually 'main' or 'master'
BASE_BRANCH=main

# ============================================================================
# OLLAMA CONFIGURATION
# ============================================================================

# Ollama host (don't change unless running external Ollama)
OLLAMA_HOST=http://ollama:11434

# ============================================================================
# REVIEW CONFIGURATION
# ============================================================================

# Output format for reviews
# Options: markdown, json, text
OUTPUT_FORMAT=markdown

# Reviews directory
REVIEWS_PATH=./reviews

# ============================================================================
# ADVANCED (usually don't need to change)
# ============================================================================

# Maximum tokens for review output
MAX_TOKENS=2000

# Temperature for AI responses (0.0-1.0, lower = more focused)
TEMPERATURE=0.3